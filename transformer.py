import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout
from tensorflow.keras.utils import to_categorical


# Load and preprocess the data
def load_and_preprocess_data(num_words=10000, maxlen=100):
    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)

    # Pad sequences to ensure uniform input size
    X_train = pad_sequences(X_train, maxlen=maxlen)
    X_test = pad_sequences(X_test, maxlen=maxlen)

    # Convert labels to categorical
    y_train = to_categorical(y_train)
    y_test = to_categorical(y_test)

    return (X_train, y_train), (X_test, y_test)


# Build a more complex LSTM model
from tensorflow.keras.layers import Embedding


def build_model(input_shape, output_size):
    model = Sequential()
    model.add(Embedding(input_dim=10000, output_dim=128, input_length=input_shape[1]))
    model.add(LSTM(128, return_sequences=True))
    model.add(LSTM(64))
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(output_size, activation='softmax'))

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model


def train_and_save_model(X_train, y_train, X_test, y_test, epochs=5, batch_size=64):
    model = build_model(X_train.shape, y_train.shape[1])

    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))

    # Save the trained model
    model.save('lstm_model.h5')


# Main execution
if __name__ == "__main__":
    (X_train, y_train), (X_test, y_test) = load_and_preprocess_data()
    train_and_save_model(X_train, y_train, X_test, y_test)



from tensorflow.keras.preprocessing.text import text_to_word_sequence
from tensorflow.keras.models import load_model


# Load and preprocess the data
def preprocess_new_review(review, maxlen=100):
    # Tokenize the review
    tokens = text_to_word_sequence(review)

    # Convert tokens to integer indices
    word_index = imdb.get_word_index()
    token_indices = [word_index.get(word, 2) for word in tokens]  # '2' for unknown tokens

    # Pad the sequence
    padded_sequence = pad_sequences([token_indices], maxlen=maxlen)
    return padded_sequence


def predict_review(model, review):
    maxlen = 100  # Same as used during training
    preprocessed_review = preprocess_new_review(review, maxlen)

    # Predict the class probabilities
    probabilities = model.predict(preprocessed_review)

    # Get the predicted class
    predicted_class_index = np.argmax(probabilities, axis=1)[0]

    # Convert index to class name
    class_names = ['negative', 'positive']  # Replace with your actual class names
    predicted_class_name = class_names[predicted_class_index]

    return predicted_class_name


# Example usage
if __name__ == "__main__":
    new_review = "This movie was fantastic! The plot was engaging and the acting was superb."
    predicted_class = predict_review(model, new_review)
    print(f"Predicted class for the new review: {predicted_class}")

def evaluate_model(model, X_test, y_test):
    loss, accuracy = model.evaluate(X_test, y_test)
    print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")

# In your training script, after saving the model
evaluate_model(model, X_test, y_test)

test_reviews = [
    "I loved the movie! It was a great experience.",
    "The movie was okay, but it could be better.",
    "I didn't like the movie at all. It was a waste of time.",
    "The plot was predictable and the acting was mediocre.",
    "An absolute masterpiece! Highly recommended."
]

for review in test_reviews:
    predicted_class = predict_review(model, review)
    print(f"Review: {review}")
    print(f"Predicted class: {predicted_class}\n")
